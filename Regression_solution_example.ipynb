{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth session\n",
    "\n",
    "# Parameters, ground-truth of W and b\n",
    "W_true = np.array([1, 2, 3, 4, 5, 6]).reshape(3, 2).astype(np.float32)\n",
    "b_true = np.array([7, 8, 9]).reshape(3, 1).astype(np.float32)\n",
    "\n",
    "# y = W * x + b\n",
    "def linear_forward(x, W=None, b=None, forward=False, W_true=W_true, b_true=b_true):\n",
    "    # Fast forward for ground-truth\n",
    "    if forward:\n",
    "        return tf.matmul(tf.Variable(W_true), x) + tf.Variable(b_true)\n",
    "    # Compute regression with input x and current W and b\n",
    "    return tf.matmul(W, x) + b\n",
    "\n",
    "# Datas\n",
    "NUM_EXAMPLES = 2000\n",
    "x_data = np.random.normal(size=(2, NUM_EXAMPLES)).astype(np.float32)\n",
    "\n",
    "# Random fetch method\n",
    "def random_fetch(x_data=x_data, batchsize=10):\n",
    "    # Shuffle on column dimension\n",
    "    np.random.shuffle(np.transpose(x_data))\n",
    "    # Cut data with batchsize\n",
    "    x = x_data[:, :batchsize]\n",
    "    # Generate noise\n",
    "    noise = np.random.normal(size=(3, batchsize)).astype(np.float32)\n",
    "    # Return x and y\n",
    "    return x, linear_forward(x, forward=True) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.98180306,  1.0938388 , -0.94574136, -0.17907181, -0.20082615,\n",
       "          0.1252659 ,  1.0211341 ,  0.4286922 , -0.19151522, -1.1704316 ],\n",
       "        [-1.108914  , -1.1972133 , -0.8098109 ,  0.1461534 , -1.0345107 ,\n",
       "         -0.22616908,  0.70177734,  0.5739198 ,  1.247847  ,  1.1756903 ]],\n",
       "       dtype=float32),\n",
       " <tf.Tensor: id=71153, shape=(3, 10), dtype=float32, numpy=\n",
       " array([[ 4.8074512 ,  5.942597  ,  4.626642  ,  7.0413265 ,  5.3954854 ,\n",
       "          5.393152  ,  8.75561   ,  9.069699  ,  8.735003  ,  8.827291  ],\n",
       "        [ 8.855062  ,  6.7373676 ,  1.2061688 ,  7.6327972 ,  3.5771933 ,\n",
       "          6.981923  , 13.357912  , 12.363074  , 11.76876   ,  8.196025  ],\n",
       "        [ 6.546202  ,  7.041901  , -0.91067636,  9.482562  ,  1.1766717 ,\n",
       "          9.792259  , 18.987152  , 13.909901  , 17.042025  , 10.993371  ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module building session\n",
    "\n",
    "# LinearModule building\n",
    "class LinearModel(tf.keras.Model):\n",
    "    # Cross init\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # Variables that are trainable\n",
    "        self.W = tf.Variable(tf.random.uniform((3, 2)), name='weight')\n",
    "        self.b = tf.Variable(tf.random.uniform((3, 1)), name='bias')\n",
    "    # 'call method' of LinearModel\n",
    "    def call(self, inputs):\n",
    "        return linear_forward(inputs, self.W, self.b)\n",
    "\n",
    "# Computation of loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "    error = model(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Computation of gradient\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    # Return current loss_value and gradient\n",
    "    return loss_value, tape.gradient(loss_value, [model.W, model.b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training session\n",
    "\n",
    "# Init LinearModuel as model\n",
    "model = LinearModel()\n",
    "# Init optimizer\n",
    "optimizer = tf.optimizers.Adam(0.1)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "# Training 1000 times\n",
    "for i in range(1000):\n",
    "    # Random fetch x and y\n",
    "    x, y = random_fetch(batchsize=100)\n",
    "    # Compute loss_value and grads\n",
    "    loss_value, grads = grad(model, x, y)\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.b]))\n",
    "    # Print loss each 100 steps\n",
    "    if i % 100 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss_value))\n",
    "\n",
    "# Print trained W and b\n",
    "print(model.W, model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
